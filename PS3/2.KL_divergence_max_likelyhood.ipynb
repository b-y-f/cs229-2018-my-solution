{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Nonnegativity\n",
    "\n",
    "<center><img src=\"jensen's_inequality.png\" width=\"800\"></center>\n",
    "\n",
    "From given formula,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "D_{KL}(P,Q)=D_{KL}(P\\|Q) &= \\sum_{x=X} P(x) \\log \\frac{P(x)}{Q(x)} \\\\\\\\\n",
    "\n",
    "&= \\sum_{x=X} P(x) \\log P(x) - \\sum_{x=X} P(x) \\log Q(x) \\\\\\\\\n",
    "\n",
    "&= - \\left(  \\sum_{x=X} P(x) \\log Q(x) - \\sum_{x=X} P(x) \\log P(x) \\right)\\\\\\\\\n",
    "\n",
    "&= - \\sum_{x=X} P(x) \\log \\frac{Q(x)}{P(x)} \\\\\\\\\n",
    "\n",
    "& \\text{ Now, let } Y \\text{ takes on values } \\frac{Q(y)}{P(y)} \\text{ with probabilities } P(y). \\text{ Then, with LOTUS} \\\\\\\\\n",
    "\n",
    "&= - E[ \\log Y] \\\\\\\\\n",
    "\n",
    "&\\text{ By Jensen's inequal :} E[f(X)] \\geq f(E[X]) \\\\\\\\\n",
    "\n",
    "& \\Rightarrow  -E[ \\log Y] \\geq -f(E[Y]) \\\\\\\\\n",
    "\n",
    "&\\text{ Because, } \\\\\\\\\n",
    "\n",
    "E[Y] &= \\sum_{y=Y} P(y) \\cdot \\frac{Q(y)}{P(y)} = \\sum_{y=Y} Q(y) = 1 \\\\\\\\\n",
    "\n",
    "&\\text{Thus, } \\\\\\\\\n",
    "\n",
    "D_{KL}(P,Q) &= -E[ \\log Y] \\geq -log(1) = 0.\n",
    "\n",
    "\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For condition $P=Q$, we will have ,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "D_{KL}(P \\| P) &= \\sum_{x=X} P \\log (\\frac{P}{P}) \\\\\\\\\n",
    "\n",
    "&= \\sum_{x=X} P \\log(1) = \\sum 0 = 0\n",
    "\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note:\n",
    "\n",
    "[Other ways to proof](https://stats.stackexchange.com/questions/335197/why-kl-divergence-is-non-negative)\n",
    "\n",
    "[LOTUS](https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) KL divergence: chain rule\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "&\\text{With given definition plus conditional probability rule: } \\\\\\\n",
    "\n",
    "D_{KL} (P(X,Y) \\| Q(X,Y)) &= \\sum_x  \\sum_y P(x,y) \\log \\frac{P(x)P(y,x)}{Q(x)Q(y|x)}  \\\\\\\\\n",
    "\n",
    "& \\text{Split them by } \\log(AB) = \\log(A)+\\log(B) \\\\\\\n",
    "\n",
    "&= \\sum_x  \\sum_y P(x,y) \\log \\frac{P(x)}{Q(x)} + \\sum_x \\sum_y P(x,y) \\log  \\frac{P(y|x)}{Q(y|x)} \\\\\\\\\n",
    "\n",
    "&\\text{We can write the marginal probability of } x \\text{ as:} P(x) = \\sum_y P(x,y). \\text{ then,}\\\\\\\n",
    "\n",
    "\n",
    "&= \\sum_x P(x) \\sum_y  \\log \\frac{P(x)}{Q(x)} + \\sum_x \\sum_y P(x,y) \\log  \\frac{P(y|x)}{Q(y|x)} \\\\\\\\\n",
    "\n",
    "&= D_{KL} (P(x) \\| Q(x)) + \\sum_x \\sum_y P(x,y) \\log  \\frac{P(y|x)}{Q(y|x)} \\\\\\\\\n",
    "\n",
    "&\\text{use conditional probability rule again, since }p(x) \\text{ not depend on y we take it out,} \\\\\\\n",
    "\n",
    "&= D_{KL} (P(x) \\| Q(x)) + \\sum_x p(x) \\sum_y P(y|x) \\log  \\frac{P(y|x)}{Q(y|x)} \\\\\\\\\n",
    "\n",
    "&= D_{KL} (P(x) \\| Q(x)) + D_{KL} (P(Y|X) \\| Q(Y|X)). \\\\\\\\\n",
    "\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Maximum Likelihood Estimation is Minimizing KL Divergence\n",
    "\n",
    "According to the definition for KL divergence:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\n",
    "\\argmin_\\theta D_{KL} (\\hat{P} \\| P_{\\theta}) &= \\sum_{x \\in X} \\hat{P}(x^{(i)}) \\log \\frac{\\hat{P}(x^{(i)})} {P_{\\theta}(x^{(i)})} \\\\\\\\\n",
    "\n",
    "&= \\sum_{x \\in X} \\hat{P}(x^{(i)}) \\log \\hat{P}(x^{(i)}) - \\sum_{x \\in X} \\hat{P}(x^{(i)}) \\log P_{\\theta}(x^{(i)}) \\\\\\\\\n",
    "\n",
    "&= \\text{const} - \\sum_{x \\in X} \\hat{P}(x^{(i)}) \\log P_{\\theta}(x^{(i)}) \\\\\\\\\n",
    "\n",
    "&= 0 - \\sum_{x \\in X} 1 \\cdot \\log P_{\\theta}(x^{(i)})  \\\\\\\\\n",
    "\n",
    "&= - \\sum_{x \\in X} \\log P_{\\theta}(x^{(i)}) \\\\\\\\\n",
    "\n",
    "&\\Rightarrow \\argmax_{\\theta} = \\sum_{x \\in X} \\log P_{\\theta}(x^{(i)}) \\\\ \\blacksquare\n",
    "\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

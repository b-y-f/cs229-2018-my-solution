{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import src.util as util\n",
    "\n",
    "from src.linear_model import LinearModel"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) : Show Poission dist is exponential family\n",
    "\n",
    "$$\\begin{align}\n",
    "p(y;\\lambda) &= \\frac{e^{-\\lambda} \\lambda ^{y}}{y!} \\\\\n",
    "&= \\exp(\\log(\\frac{\\lambda ^{y} e^{-\\lambda} }{y!})) \\\\\n",
    "&= \\exp(y\\log(\\lambda ) -\\lambda  - \\log(y!)) \\\\\n",
    "&= \\frac{1}{y!} \\exp(y\\log(\\lambda ) -\\lambda ) \\\\\n",
    "\\\\\n",
    "&\\text{Then we put into cannoical form} : b(y) \\exp(\\eta T(y) -a(\\eta)) \\\\\n",
    "\\\\ \n",
    "&=  \\frac{1}{y!} \\exp(\\eta y -a(\\lambda) ) \\\\ \\\\\n",
    "&\\text{Where} \\\\ \\\\\n",
    "&b(y) = \\frac{1}{y!} , \\eta = \\log(\\lambda) , T(y)=y, a(\\eta)= e^{\\eta} = \\lambda\n",
    "\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "### Note:\n",
    "\n",
    "__*__ why $\\exp(-\\log(y)) = 1/y$:\n",
    "\n",
    "1. Let's start with the definition of the natural logarithm: $\\log(y) = x$ if and only if $e^x = y$.\n",
    "2. From this definition, we can see that $e^{\\log(y)} = y$.\n",
    "3. Now, let's take the reciprocal of both sides: $\\frac{1}{e^{\\log(y)}} = \\frac{1}{y}$.\n",
    "4. Using the property that $a^{-b} = \\frac{1}{a^b}$, we can rewrite the left side as $e^{-\\log(y)} = \\frac{1}{y}$.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) : find the canonical response function for poission family\n",
    "\n",
    "Canonical response function's inverse, $g^{âˆ’1}$, is called the **canonical link function**, thus we need to find canonical link function then reverse it to get the answer.\n",
    "\n",
    "The **canonical link function** is the default link function used in a generalized linear model (GLM) for a particular distribution. It relates the expected value of the response variable to the linear predictor. For distributions in the exponential family, the canonical link function is the function that relates the mean of the distribution to the natural parameter of the distribution.\n",
    "\n",
    "The **inverse link function**, on the other hand, is simply the inverse of the link function. It maps the linear predictor back to the expected value of the response variable. The inverse of the canonical link function is called the **canonical response function**.\n",
    "\n",
    "Poisson regression, where the response variable follows a Poisson distribution, the canonical link function is the logarithm and the canonical response function (i.e., the inverse link function) is the exponential function. This means that if we have a linear predictor $\\eta = \\theta^TX$, where $X$ is a matrix of predictor variables and $\\theta$ is a vector of coefficients, then we can write $\\log(\\mu) = \\eta$, where $\\mu$ is the expected value of the response variable. The inverse link function maps $\\eta$ back to $\\mu$, so we have $\\mu = \\exp(\\eta)$.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) derive stochastic gradient ascent update rule\n",
    "\n",
    "We can write the probability function as :\n",
    "\n",
    "$$\n",
    "p(y^{(i)}| x^{(i)}, \\theta) = \\prod^{n}_{i=1} \\frac{\\exp(y^{(i)} \\theta^T x^{(i)} - e^{\\theta^T x^{(i)}})}{y^{(i)}!}\n",
    "$$\n",
    "\n",
    "then take the log likelyhood we convert product into sum:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(y^{(i)} | x^{(i)}, \\theta) = \\sum^{n}_{i=1} ( y^{(i)} \\theta^T x^{(i)}  - \\exp( \\theta^T x^{(i)}) - \\log(y^{(i)}!) )\n",
    "$$\n",
    "\n",
    "to get maximum likelyhood with respect to $\\theta$, we derivetive the likelyhood function w.r.t $\\theta$ and set to 0:\n",
    "\n",
    "$$\\begin{align}\n",
    "\\frac{\\partial \\mathcal{L} (y^{(i)} | x^{(i)}, \\theta)}{\\partial \\theta} &= \\sum^{n}_{i=1} (y^{(i)} x^{(i)} - \\exp( \\theta^T x^{(i)}) x^{(i)}) \\\\\n",
    "&= \\sum^{n}_{i=1} (y^{(i)} - e^{( \\theta^T x^{(i)})}) x^{(i)} = 0\n",
    "\\end{align}$$\n",
    "\n",
    "However, this equation cannot be solved analytically for $\\theta$. Instead, numerical methods such as Newton-Raphson or gradient descent are typically used to find the maximum likelihood estimate of $\\theta$. Thus, this equation can be write as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L} (y^{(i)} | x^{(i)}, \\theta)}{\\partial \\theta} = \\sum^{n}_{i=1} (y^{(i)} - \\hat{y}^{(i)}) x^{(i)} = gradient\n",
    "$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) implement Poission regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1942.9024896371197\n"
     ]
    }
   ],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "\n",
    "\n",
    "def prob_d(lr, train_path, eval_path, pred_path):\n",
    "    \"\"\"Problem 3(d): Poisson regression with gradient ascent.\n",
    "\n",
    "    Args:\n",
    "        lr: Learning rate for gradient ascent.\n",
    "        train_path: Path to CSV file containing dataset for training.\n",
    "        eval_path: Path to CSV file containing dataset for evaluation.\n",
    "        pred_path: Path to save predictions.\n",
    "    \"\"\"\n",
    "    # Load training set\n",
    "    x_train, y_train = util.load_dataset(train_path, add_intercept=False)\n",
    "    x_eval, y_eval = util.load_dataset(eval_path, add_intercept=False)\n",
    "    # *** START CODE HERE ***\n",
    "    model = PoissonRegression(step_size=lr, max_iter=10**3)\n",
    "    model.fit(x_train, y_train)\n",
    "    preds = model.predict(x_eval)\n",
    "    print(rmse(y_eval, preds))\n",
    "    # *** END CODE HERE ***\n",
    "\n",
    "\n",
    "class PoissonRegression(LinearModel):\n",
    "    \"\"\"Poisson Regression.\n",
    "\n",
    "    Example usage:\n",
    "        > clf = PoissonRegression(step_size=lr)\n",
    "        > clf.fit(x_train, y_train)\n",
    "        > clf.predict(x_eval)\n",
    "    \"\"\"\n",
    "    def fit(self, x, y):\n",
    "        \"\"\"Run gradient ascent to maximize likelihood for Poisson regression.\n",
    "\n",
    "        Args:\n",
    "            x: Training example inputs. Shape (m, n).\n",
    "            y: Training example labels. Shape (m,).\n",
    "        \"\"\"\n",
    "        # *** START CODE HERE ***\n",
    "    \n",
    "        m, n = x.shape\n",
    "        if self.theta == None:\n",
    "            self.theta =np.random.rand(n)\n",
    "            # self.theta = np.zeros(n) \n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            y_hat = np.exp(x@self.theta)\n",
    "            grad = x.T @ (y - y_hat)\n",
    "            lr = self.step_size * grad\n",
    "            self.theta += lr\n",
    "        # *** END CODE HERE ***\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Make a prediction given inputs x.\n",
    "\n",
    "        Args:\n",
    "            x: Inputs of shape (m, n).\n",
    "\n",
    "        Returns:\n",
    "            Floating-point prediction for each input, shape (m,).\n",
    "        \"\"\"\n",
    "        # *** START CODE HERE ***\n",
    "        y_hat = np.exp(x @ self.theta)\n",
    "        return y_hat\n",
    "        # *** END CODE HERE ***\n",
    "\n",
    "\n",
    "prob_d(8.5e-11, \"./data/ds4_train.csv\",\"./data/ds4_valid.csv\",\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lake-influx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

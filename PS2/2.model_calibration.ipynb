{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) \n",
    "\n",
    "Prove that :\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{i \\in a,b} P(y^{(i)} = 1 | x^{(i)}; \\theta)}{|{i \\in I_{a,b}}|} = \\frac{\\sum_{i \\in a,b} \\Bbb{I}(y^{(i)} = 1)}{|{i \\in I_{a,b}}|}\n",
    "$$\n",
    "\n",
    "or \n",
    "\n",
    "$$\n",
    "\\sum_{i \\in a,b} P(y^{(i)} = 1 | x^{(i)}; \\theta) = \\sum_{i \\in a,b} \\Bbb{I}(y^{(i)} = 1)\n",
    "$$\n",
    "\n",
    "From the loss function :\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m [y^{(i)} \\log(h_\\theta(x^{(i)})) + (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)}))]\n",
    "$$\n",
    "\n",
    "\n",
    "To get best $\\theta$ we need to set derivative of $J(\\theta)$ w.r.t $\\theta$ to zero:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} &= \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} =0 \\\\\\\\\n",
    "&\\Rightarrow \\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} =0 \\\\\\\\\n",
    "&\\Rightarrow \\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)}) =0 \\\\\\\\\n",
    "&\\Rightarrow \\sum_{i=1}^mh_\\theta(x^{(i)})  = \\sum_{i=1}^m y^{(i)} \\\\\\\\\n",
    "&\\Rightarrow \\sum_{i \\in a,b} P(y^{(i)} = 1 | x^{(i)}; \\theta) = \\sum_{i \\in a,b} \\Bbb{I}(y^{(i)} = 1)\n",
    "\\end{align*} $$\n",
    "\n",
    "Thus, L2 regulation will make the prediction result less accurate with real.\n",
    "\n",
    "------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b)\n",
    "\n",
    "A: Not true and converse not true neither.\n",
    "\n",
    "Assume the model is in perfect calibration. \n",
    "\n",
    "Let range of probability not (0,1) rather (0,0.1), then $0 < P(y^{(i)}) <0.1$, which means:\n",
    "\n",
    "$$\n",
    "\\sum^m_{i=1} P(y^{(i)} =1) < |{i \\in I_{a,b}}|\n",
    "$$\n",
    "\n",
    "But at the meantime:\n",
    "\n",
    "$$ \\sum^m_{i=1} \\Bbb{I}(y^{(i)}=1) = |{i \\in I_{a,b}}| $$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{i \\in 0, 0.1} P(y^{(i)} = 1 | x^{(i)}; \\theta)}{|{i \\in I_{a,b}}|} < \\frac{\\sum_{i \\in 0,1} \\Bbb{I}(y^{(i)} = 1)}{|{i \\in I_{a,b}}|}\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c)\n",
    "\n",
    "\n",
    "Sure! To add L2 regularization to the logistic regression loss function, we can modify the original loss function as follows:\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m [y^{(i)} \\log(h_\\theta(x^{(i)})) + (1-y^{(i)}) \\log(1-h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m}\\sum_{j=1}^n \\theta_j^2\n",
    "$$\n",
    "\n",
    "The derivative of the regularized logistic regression loss function $J(\\theta)$ with respect to $\\theta_j$ is given by:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} &= \\frac{1}{m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} + \\frac{\\lambda}{m}\\theta_j  = 0 \\\\\\\\\n",
    "& \\Rightarrow  \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} + \\lambda\\theta_j  = 0 \\\\\\\\\n",
    "& \\Rightarrow  \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_j^{(i)} + \\lambda\\theta_j  = 0 \\\\\\\\\n",
    "& \\text {Let } \\theta_j = 0 \\text{ then:} \\\\\\\\\n",
    "& \\Rightarrow  \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})x_0^{(i)} + \\lambda\\theta_0  = 0 \\\\\\\\\n",
    "& \\Rightarrow  \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})*1 + \\lambda\\theta_0  = 0 \\\\\\\\\n",
    "& \\Rightarrow  \\sum_{i=1}^m h_\\theta(x^{(i)}) - \\sum_{i=1}^m y^{(i)} + \\lambda\\theta_0  = 0 \\\\\\\\\n",
    "& \\Rightarrow  \\sum_{i=1}^m h_\\theta(x^{(i)}) + \\lambda\\theta_0 = \\sum_{i=1}^m y^{(i)}  \\\\\\\\\n",
    "\\end{align*} $$\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
